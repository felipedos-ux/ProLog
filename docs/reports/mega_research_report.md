# üî¨ Mega Relat√≥rio: Log Anomaly Detection com LogGPT

**Data**: 2026-02-07  
**Projeto**: ProLog - Universal Log Anomaly Detection  
**Status**: Em desenvolvimento, enfrentando problemas de generaliza√ß√£o

---

## üìã √çndice

1. [Contexto e Objetivo](#1-contexto-e-objetivo)
2. [O Que Foi Feito](#2-o-que-foi-feito)
3. [Resultados no OpenStack (Sucesso)](#3-resultados-no-openstack-sucesso)
4. [Resultados no BGL (Falha)](#4-resultados-no-bgl-falha)
5. [An√°lise Comparativa](#5-an√°lise-comparativa)
6. [Problemas Identificados](#6-problemas-identificados)
7. [Tentativas de Solu√ß√£o](#7-tentativas-de-solu√ß√£o)
8. [Perguntas para Pesquisa Externa](#8-perguntas-para-pesquisa-externa)

---

## 1. Contexto e Objetivo

### 1.1 O Problema

Sistemas computacionais geram milh√µes de logs diariamente. Detectar anomalias nesses logs √© crucial para:
- Identificar falhas antes que causem interrup√ß√µes
- Detectar intrus√µes de seguran√ßa
- Monitorar sa√∫de do sistema

### 1.2 Abordagem Escolhida

Implementar um **detector universal de anomalias** baseado em **LogGPT** (Language Model) que:
1. Aprende padr√µes normais de execu√ß√£o
2. Detecta desvios como anomalias
3. Funciona em diferentes datasets sem retreino

### 1.3 Arquitetura LogGPT-Small

```
Modelo: GPT-style Transformer (Decoder-only)
- Layers: 4
- Heads: 4  
- Embedding: 256
- Block Size: 128
- Vocab: 50,357 (GPT-2 tokenizer)
- Par√¢metros: ~3M
```

### 1.4 Datasets Utilizados

| Dataset | Origem | Logs | Anomalias | Tipo |
|---------|--------|------|-----------|------|
| **OpenStack** | Cloud Computing | ~200K | ~5% | IaaS operations |
| **BGL** | Blue Gene/L Supercomputer | ~4.7M | ~7.3% | HPC messages |

---

## 2. O Que Foi Feito

### 2.1 Fase 1: Treinamento no OpenStack

**Objetivo**: Treinar LogGPT-Small em sequ√™ncias normais do OpenStack.

**Processo**:
1. Pr√©-processamento com Drain para extra√ß√£o de templates
2. Agrupamento por sess√£o (BlockId)
3. Treino com language modeling (next-token prediction)
4. 100 epochs, LR=1e-4, Batch=16

**Resultado**: ‚úÖ **Sucesso** - F1 > 0.95 no OpenStack

### 2.2 Fase 2: Avalia√ß√£o no BGL (Sem Retreino)

**Objetivo**: Validar se o modelo treinado no OpenStack generaliza para BGL.

**Processo**:
1. Pr√©-processamento do BGL com sliding window (20 eventos)
2. Avalia√ß√£o usando perplexidade como sinal de anomalia
3. Threshold autom√°tico no validation set

**Resultado**: ‚ùå **Falha** - F1 = 0.65, TN = 0 (todos normais marcados como anomalias)

### 2.3 Fase 3: Fine-tuning no BGL

**Objetivo**: Fine-tune do modelo no BGL para ver se melhora.

**Processo**:
1. Fine-tune com 5,000 sequ√™ncias normais do BGL
2. 100 epochs, LR=1e-6 (par√¢metros do paper LogGPT)
3. Converg√™ncia excelente (loss 3.65 ‚Üí 0.025)

**Resultado**: ‚ùå **Falha** - F1 = 0.65 (id√™ntico ao modelo universal)

### 2.4 Fase 4: Universal Detector Multi-Signal

**Objetivo**: Combinar m√∫ltiplos sinais al√©m de perplexidade.

**Sinais Implementados**:
- **Perplexity**: Qu√£o "surpreendente" √© a sequ√™ncia para o modelo
- **Rarity**: Frequ√™ncia dos templates (templates raros = mais an√¥malos)
- **Context**: Consist√™ncia contextual da sequ√™ncia

**Resultado**: ‚ùå **Falha** - F1 = 0.65, calibra√ß√£o autom√°tica falhou

---

## 3. Resultados no OpenStack (Sucesso)

### 3.1 M√©tricas Alcan√ßadas

| M√©trica | Valor | Target |
|---------|-------|--------|
| **Precision** | 0.96 | > 0.90 ‚úÖ |
| **Recall** | 0.94 | > 0.90 ‚úÖ |
| **F1 Score** | 0.95 | > 0.90 ‚úÖ |
| **Accuracy** | 0.97 | > 0.95 ‚úÖ |

### 3.2 Por Que Funcionou

1. **Dataset Homog√™neo**: OpenStack tem padr√µes claros e repetitivos
2. **Vocabul√°rio Limitado**: ~500 templates √∫nicos
3. **Sess√µes Bem Definidas**: BlockId agrupa opera√ß√µes relacionadas
4. **Separa√ß√£o Clara**: Perplexidade de anomalias >> perplexidade de normais

### 3.3 Distribui√ß√£o de Perplexidade no OpenStack

```
Normal:    [===1.0---2.0---3.0===]  Œº=2.0, œÉ=0.5
Anomaly:            [===6.0---8.0---10.0===]  Œº=8.0, œÉ=1.5
                                              
Separa√ß√£o: ŒîŒº = 6.0 (EXCELENTE)
```

---

## 4. Resultados no BGL (Falha)

### 4.1 M√©tricas Obtidas

| M√©trica | Universal | Fine-tuned | Target |
|---------|-----------|------------|--------|
| **Precision** | 0.489 | 0.489 | > 0.70 ‚ùå |
| **Recall** | 1.000 | 1.000 | > 0.80 ‚úÖ |
| **F1 Score** | 0.657 | 0.657 | > 0.70 ‚ùå |
| **TN** | 0 | 0 | > 0 ‚ùå |
| **FP** | 511 | 511 | < 100 ‚ùå |

### 4.2 O Problema Central

**Threshold = 0.00** ‚Üí Todos os casos s√£o marcados como anomalias!

Isso significa que o algoritmo de busca de threshold encontrou que a **melhor** estrat√©gia √© marcar **TUDO** como anomalia, porque:
- Recall = 100% (todas anomalias detectadas)
- Precision = 48.9% (propor√ß√£o de anomalias no dataset)
- F1 = 65.7% (melhor que qualquer threshold positivo)

### 4.3 Distribui√ß√£o de Perplexidade no BGL

**Modelo Universal (OpenStack)**:
```
Normal:    [===2.7---3.4---4.1===]  Œº=3.41, œÉ=0.36
Anomaly:         [===3.3---4.5---5.7===]  Œº=4.51, œÉ=0.62
                      ^^^ OVERLAP ^^^
Separa√ß√£o: ŒîŒº = 1.10 (INSUFICIENTE)
```

**Modelo Fine-tuned (BGL)**:
```
Normal:    [===0.4---1.1---1.8===]  Œº=1.10, œÉ=0.36
Anomaly:         [===0.8---3.0---5.1===]  Œº=2.98, œÉ=1.08
                      ^^^ OVERLAP ^^^
Separa√ß√£o: ŒîŒº = 1.88 (AINDA INSUFICIENTE)
```

### 4.4 Por Que N√£o Funcionou

1. **Vocabul√°rio Muito Maior**: BGL tem 242 templates √∫nicos vs ~500 do OpenStack
2. **Templates Diferentes**: Nenhum overlap entre templates BGL e OpenStack
3. **OOV (Out-of-Vocabulary)**: Todos templates BGL s√£o "desconhecidos" para o modelo
4. **Sobreposi√ß√£o de Distribui√ß√µes**: Normal e anomalia t√™m PPL similar
5. **Dataset Desbalanceado**: 48.9% anomalias vs 51.1% normais

---

## 5. An√°lise Comparativa

### 5.1 Diferen√ßas Estruturais entre Datasets

| Caracter√≠stica | OpenStack | BGL |
|----------------|-----------|-----|
| **Origem** | Cloud IaaS | Supercomputer |
| **Templates √önicos** | ~500 | 242 |
| **Overlap de Vocabul√°rio** | - | 0% |
| **Formato de Sess√£o** | BlockId | Sliding Window |
| **Taxa de Anomalia** | ~5% | 48.9% (nas amostras) |
| **Tipo de Log** | Opera√ß√µes CRUD | Mensagens de sistema |
| **Estrutura** | `[timestamp] [level] message` | `[timestamp] [node] [type] message` |

### 5.2 Exemplo de Logs

**OpenStack**:
```
2024-01-15 10:23:45 INFO nova.compute.manager [req-abc] Starting instance i-12345
2024-01-15 10:23:46 INFO nova.compute.manager [req-abc] Instance i-12345 spawned successfully
```

**BGL**:
```
1117838570 2005.06.03 R02-M1-N0-C:J12-U11 RAS KERNEL INFO generating core.12345
1117838570 2005.06.03 R02-M1-N0-C:J12-U11 RAS KERNEL FATAL double hummer exception
```

### 5.3 Por Que a Transfer√™ncia N√£o Funciona

```mermaid
flowchart LR
    subgraph OpenStack
        A[Template: "Starting instance <*>"] --> B[Tokenizado: ID 1234]
        C[Template: "Instance <*> spawned"] --> D[Tokenizado: ID 5678]
    end
    
    subgraph BGL
        E[Template: "generating core.<*>"] --> F[Tokenizado: IDs desconhecidos]
        G[Template: "double hummer exception"] --> H[Tokenizado: IDs desconhecidos]
    end
    
    B & D --> I[Modelo aprende padr√µes]
    F & H --> J[Modelo n√£o reconhece]
    
    I --> K[‚úÖ Boa predi√ß√£o]
    J --> L[‚ùå Alta perplexidade para TUDO]
```

---

## 6. Problemas Identificados

### 6.1 Problema 1: Zero Transfer Learning

**Descri√ß√£o**: O modelo treinado no OpenStack n√£o transfere conhecimento para BGL.

**Evid√™ncia**:
- Perplexidade alta para TODOS os logs do BGL (normais e an√¥malos)
- Nenhum template do BGL aparece no vocabul√°rio aprendido

**Causa Prov√°vel**:
- Tokeniza√ß√£o baseada em texto (GPT-2 tokenizer) n√£o captura sem√¢ntica de logs
- Dom√≠nios completamente diferentes (cloud vs HPC)

### 6.2 Problema 2: Perplexidade N√£o Discrimina

**Descri√ß√£o**: Mesmo ap√≥s fine-tuning, perplexidade n√£o separa normal de anomalia.

**Evid√™ncia**:
- Universal: Normal PPL 3.41, Anomaly PPL 4.51 (Œî=1.10)
- Fine-tuned: Normal PPL 1.10, Anomaly PPL 2.98 (Œî=1.88)
- Overlap significativo em ambos

**Causa Prov√°vel**:
- Anomalias no BGL n√£o s√£o "linguisticamente diferentes" dos normais
- Anomalias s√£o definidas por padr√µes SEQUENCIAIS, n√£o por tokens individuais

### 6.3 Problema 3: Calibra√ß√£o Autom√°tica Falha

**Descri√ß√£o**: O sistema de calibra√ß√£o autom√°tica de threshold converge para 0.

**Evid√™ncia**:
- Melhor F1 ocorre com threshold=0 (marcar tudo como anomalia)
- Nenhum threshold positivo melhora o F1

**Causa Prov√°vel**:
- Dataset altamente desbalanceado (48.9% anomalias)
- Sobreposi√ß√£o de distribui√ß√µes impede separa√ß√£o

### 6.4 Problema 4: Sliding Window Pode Ser Inadequada

**Descri√ß√£o**: Janelas de 20 eventos podem n√£o capturar contexto suficiente.

**Evid√™ncia**:
- Papers usam janelas maiores (60 eventos) ou baseadas em tempo (1 hora)
- Janelas curtas fragmentam padr√µes de erro

**Causa Prov√°vel**:
- BGL tem logs de alta frequ√™ncia (milh√µes de mensagens)
- Erros se propagam por v√°rias mensagens consecutivas

### 6.5 Problema 5: Sinal √önico Insuficiente

**Descri√ß√£o**: Usar apenas perplexidade como sinal n√£o √© suficiente para BGL.

**Evid√™ncia**:
- Multi-signal (perplexity + rarity + context) tamb√©m falhou
- Calibra√ß√£o autom√°tica de pesos convergiu para {context: 0.8, outros: 0.1}

**Causa Prov√°vel**:
- BGL requer an√°lise de padr√µes temporais, n√£o apenas lingu√≠sticos
- Anomalias s√£o detect√°veis por sequ√™ncia de eventos, n√£o eventos individuais

---

## 7. Tentativas de Solu√ß√£o

### 7.1 Tentativa 1: Multi-Signal Fusion ‚ùå

**O que fizemos**:
- Implementar 3 sinais: perplexity, rarity, context
- Calibra√ß√£o autom√°tica de pesos no validation set
- Fus√£o ponderada dos sinais

**Resultado**:
- Pesos calibrados: {perplexity: 0.1, rarity: 0.1, context: 0.8}
- F1 = 0.657 (sem melhoria)
- TN = 0 (todos normais marcados como anomalias)

**Por que falhou**:
- Sinal de context tamb√©m n√£o discrimina bem
- Calibra√ß√£o autom√°tica convergiu para solu√ß√£o degenerada

### 7.2 Tentativa 2: Fine-tuning no BGL ‚ùå

**O que fizemos**:
- Fine-tune com 5,000 sequ√™ncias normais
- 100 epochs, LR=1e-6 (par√¢metros do paper)
- Converg√™ncia excelente (loss 3.65 ‚Üí 0.025)

**Resultado**:
- Perplexidade reduzida (Normal: 3.41‚Üí1.10, Anomaly: 4.51‚Üí2.98)
- F1 = 0.657 (id√™ntico ao universal)
- Threshold ainda = 0

**Por que falhou**:
- Fine-tuning reduziu PPL absoluta mas n√£o melhorou separabilidade
- Modelo aprendeu BGL, mas anomalias ainda t√™m PPL similar a normais

### 7.3 Tentativa 3: Ajuste de Window Size ‚è≥

**O que planejamos**:
- Testar janelas de 60 eventos (como no paper LogADEmpirical)
- Testar janelas baseadas em tempo (1 hora)

**Status**: N√£o executado ainda

### 7.4 Tentativa 4: Pesos Manuais ‚è≥

**O que planejamos**:
- Testar pesos manuais: {perplexity: 0.5, rarity: 0.3, context: 0.2}
- Normalizar sinais antes da fus√£o

**Status**: N√£o executado ainda

---

## 8. Perguntas para Pesquisa Externa

### üî¥ Perguntas Cr√≠ticas (Prioridade Alta)

#### P1: Como papers de refer√™ncia avaliam LogGPT no BGL?

**Contexto**: O paper do LogGPT reporta resultados no BGL, mas n√£o conseguimos reproduzir.

**O que preciso saber**:
1. Qual pr√©-processamento exato usam para BGL?
2. Qual tamanho de janela (window size) usam?
3. Como definem o threshold de anomalia?
4. Usam apenas perplexidade ou m√∫ltiplos sinais?
5. Qual F1 reportam no BGL?

#### P2: Qual a metodologia de avalia√ß√£o padr√£o para BGL?

**Contexto**: Diferentes papers usam metodologias diferentes.

**O que preciso saber**:
1. Sliding window vs session-based: qual √© o padr√£o?
2. Tamanho de janela recomendado (10, 20, 60 eventos)?
3. Como lidam com janelas sobrepostas?
4. Usam valida√ß√£o temporal (train antes de test)?

#### P3: Como outros detectores LLM-based tratam templates OOV?

**Contexto**: Templates do BGL s√£o completamente diferentes do OpenStack.

**O que preciso saber**:
1. Usam tokeniza√ß√£o baseada em template ou em caractere?
2. Como lidam com templates nunca vistos no treino?
3. Aplicam smoothing para templates raros?
4. Usam embeddings sem√¢nticos ao inv√©s de tokeniza√ß√£o?

### üü° Perguntas Importantes (Prioridade M√©dia)

#### P4: Qual √© o state-of-the-art para BGL atualmente?

**Contexto**: Precisamos de um baseline para comparar.

**O que preciso saber**:
1. Top 3 m√©todos com melhor F1 no BGL (2023-2024)
2. Quais sinais/features usam?
3. Usam deep learning ou m√©todos cl√°ssicos?
4. C√≥digo dispon√≠vel para reprodu√ß√£o?

#### P5: Como funciona a detec√ß√£o de padr√µes sequenciais em logs?

**Contexto**: Anomalias no BGL parecem ser sequenciais, n√£o pontuais.

**O que preciso saber**:
1. M√©todos que detectam sequ√™ncias an√¥malas (n√£o eventos individuais)
2. DeepLog, LogAnomaly, LogRobust: como lidam com sequ√™ncias?
3. Attention weights podem indicar anomalias?
4. Autoencoders sequenciais s√£o melhores que LLMs?

#### P6: Fine-tuning vs Treino from Scratch: qual √© melhor para logs?

**Contexto**: Fine-tuning n√£o melhorou nossos resultados.

**O que preciso saber**:
1. Papers comparam fine-tuning vs treino do zero?
2. Qual learning rate ideal para fine-tuning em logs?
3. Quantas amostras s√£o necess√°rias para fine-tuning efetivo?
4. Transfer learning de logs funciona entre dom√≠nios diferentes?

### üü¢ Perguntas Explorat√≥rias (Prioridade Baixa)

#### P7: Embedding-based detection √© melhor que perplexity-based?

**Contexto**: Perplexidade n√£o est√° funcionando.

**O que preciso saber**:
1. M√©todos que usam embeddings ao inv√©s de perplexidade
2. Clustering de embeddings para detectar anomalias
3. Dist√¢ncia de embeddings como sinal de anomalia
4. BERT vs GPT: qual gera melhores embeddings para logs?

#### P8: Existe um "universal log detector" que funciona em m√∫ltiplos datasets?

**Contexto**: Nosso objetivo √© um detector universal.

**O que preciso saber**:
1. Papers que avaliam em m√∫ltiplos datasets (BGL, HDFS, OpenStack, Thunderbird)
2. M√©todos que n√£o requerem retreino por dataset
3. Zero-shot ou few-shot detection em logs
4. T√©cnicas de domain adaptation para logs

#### P9: Como o LogADEmpirical avalia m√©todos no BGL?

**Contexto**: Paper de benchmark importante.

**O que preciso saber**:
1. Configura√ß√£o exata para BGL (window size, step size)
2. Resultados de DeepLog, LogAnomaly, LogRobust no BGL
3. Pr√©-processamento (Drain parameters, grouping strategy)
4. Splits de treino/val/test

#### P10: T√©cnicas de data augmentation para logs an√¥malos

**Contexto**: Dataset desbalanceado (48.9% anomalias nas amostras).

**O que preciso saber**:
1. SMOTE para logs funciona?
2. Como gerar logs an√¥malos sint√©ticos?
3. Oversampling vs undersampling: qual √© melhor?
4. Contrastive learning para logs: papers relevantes?

---

## üìä Resumo do Status Atual

| Componente | Status | Problema |
|------------|--------|----------|
| LogGPT-Small | ‚úÖ Implementado | - |
| Treino OpenStack | ‚úÖ F1=0.95 | - |
| Avalia√ß√£o BGL | ‚ùå F1=0.65 | TN=0, threshold=0 |
| Fine-tuning BGL | ‚ùå F1=0.65 | N√£o melhorou |
| Multi-Signal | ‚ùå F1=0.65 | Calibra√ß√£o falhou |
| Universal Detector | ‚ùå N√£o funciona | N√£o generaliza |

---

## üéØ O Que Esperamos da Pesquisa

Com as respostas √†s perguntas acima, esperamos:

1. **Identificar o gap metodol√≥gico**: O que estamos fazendo diferente dos papers?
2. **Validar nossa abordagem**: Perplexidade √© realmente a m√©trica certa?
3. **Encontrar alternativas**: Quais outros sinais/m√©todos funcionam no BGL?
4. **Definir pr√≥ximos passos**: Ajustar janela? Mudar para embeddings? Usar outro modelo?

---

## üìö Refer√™ncias que J√° Consultamos

1. **LogGPT** (arXiv 2302.07714): Par√¢metros de treino
2. **DeepLog**: Arquitetura LSTM, window size 10
3. **LogAnomaly**: Template2Vec, LSTM
4. **LogADEmpirical**: Benchmark de m√∫ltiplos m√©todos

---

*Documento gerado para auxiliar pesquisa externa. Aguardando respostas para prosseguir com desenvolvimento.*
