# -*- coding: utf-8 -*-
"""HTML template for the comprehensive TCC report."""

def build_html(c_metrics, c_cm, c_radar, c_lt, c_tmpl, c_pipe, OS, HD, BG):
    def fmt(m):
        if m is None: return "‚Äî"
        if abs(m)<1: return f"{m*60:.1f}s"
        if abs(m)<60: return f"{m:.1f}min"
        if abs(m)<1440: return f"{m/60:.1f}h"
        return f"{m/1440:.1f}d"

    CSS = """
    :root{--bg:#0a0a1a;--surface:#151530;--card:#1c1c40;--emerald:#27ae60;--blue:#3498db;--red:#e74c3c;--gold:#f39c12;--text:#e0e0e0;--muted:#7f8c8d;}
    *{box-sizing:border-box;margin:0;padding:0}
    body{font-family:'Inter',sans-serif;background:var(--bg);color:var(--text);line-height:1.8}
    .hero{background:linear-gradient(135deg,#0f2027,#203a43,#2c5364);padding:80px 20px;text-align:center;border-bottom:4px solid var(--emerald)}
    .hero h1{font-size:3rem;font-weight:900;background:linear-gradient(90deg,#27ae60,#3498db,#9b59b6);-webkit-background-clip:text;-webkit-text-fill-color:transparent}
    .hero p{font-size:1.2rem;color:rgba(255,255,255,.7);margin-top:10px}
    .hero .sub{font-size:.85rem;color:rgba(255,255,255,.4);margin-top:8px}
    .container{max-width:1200px;margin:0 auto;padding:40px 20px}
    section{background:var(--surface);border-radius:16px;padding:45px;margin-bottom:40px;border:1px solid rgba(255,255,255,.05);box-shadow:0 8px 32px rgba(0,0,0,.3)}
    h2{color:var(--emerald);font-weight:800;font-size:1.8rem;border-bottom:2px solid rgba(39,174,96,.3);padding-bottom:12px;margin-bottom:25px}
    h3{color:var(--blue);font-weight:700;font-size:1.3rem;margin:30px 0 15px}
    h4{color:var(--gold);font-weight:600;margin:20px 0 10px}
    p{margin-bottom:15px;font-size:15px}
    .img-c{text-align:center;margin:25px 0}
    .img-c img{max-width:100%;border-radius:12px;border:1px solid rgba(255,255,255,.1)}
    table{width:100%;border-collapse:collapse;margin:20px 0;font-size:14px}
    th{background:rgba(39,174,96,.15);color:var(--emerald);padding:14px 12px;text-align:left;font-weight:700;border-bottom:2px solid rgba(39,174,96,.3)}
    td{padding:12px;border-bottom:1px solid rgba(255,255,255,.05)}
    tr:hover{background:rgba(255,255,255,.03)}
    .kpi-row{display:flex;gap:20px;flex-wrap:wrap;margin:25px 0}
    .kpi{flex:1;min-width:140px;background:var(--card);padding:22px;border-radius:12px;text-align:center;border:1px solid rgba(255,255,255,.08)}
    .kpi .v{font-size:2rem;font-weight:800;margin:6px 0}
    .kpi .l{font-size:.72rem;text-transform:uppercase;letter-spacing:1.5px;color:var(--muted);font-weight:600}
    .green .v{color:var(--emerald)}.blue .v{color:var(--blue)}.red .v{color:var(--red)}.gold .v{color:var(--gold)}
    .note{background:rgba(52,152,219,.1);border-left:4px solid var(--blue);padding:15px 20px;border-radius:0 8px 8px 0;margin:20px 0;font-size:14px}
    .warn{background:rgba(231,76,60,.1);border-left:4px solid var(--red);padding:15px 20px;border-radius:0 8px 8px 0;margin:20px 0;font-size:14px}
    .badge{display:inline-block;padding:3px 10px;border-radius:12px;font-size:11px;font-weight:700}
    .bg{background:rgba(39,174,96,.2);color:#27ae60}.br{background:rgba(231,76,60,.2);color:#e74c3c}.bb{background:rgba(52,152,219,.2);color:#3498db}
    code{background:rgba(255,255,255,.1);padding:2px 8px;border-radius:4px;font-size:12px;font-family:'Fira Code',monospace}
    ol,ul{margin:10px 0 15px 25px}
    li{margin-bottom:8px}
    .flex{display:flex;gap:30px;flex-wrap:wrap}.col{flex:1;min-width:280px}
    .step-box{background:var(--card);border-radius:12px;padding:20px;margin:15px 0;border-left:4px solid var(--emerald)}
    .step-box h4{margin-top:0;color:var(--emerald)}
    @media(max-width:800px){.flex{flex-direction:column}.kpi-row{flex-direction:column}}
    """

    return f"""<!DOCTYPE html>
<html lang="pt-BR"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>LogGPT ‚Äî Relat√≥rio Completo TCC</title>
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700;800;900&display=swap" rel="stylesheet">
<style>{CSS}</style></head><body>

<div class="hero">
<h1>üß† LogGPT: Detec√ß√£o Proativa de Anomalias em Logs</h1>
<p>Trabalho de Conclus√£o de Curso ‚Äî Relat√≥rio T√©cnico Completo</p>
<p class="sub">An√°lise comparativa em 3 datasets: OpenStack ¬∑ HDFS ¬∑ BGL | Modelo baseado em GPT-2 (Causal Language Model)</p>
</div>

<div class="container">

<!-- 1. INTRODU√á√ÉO -->
<section>
<h2>1. Introdu√ß√£o e Contexto</h2>
<p>Sistemas de software modernos ‚Äî como plataformas de <strong>cloud computing</strong>, sistemas de <strong>armazenamento distribu√≠do</strong> e <strong>supercomputadores</strong> ‚Äî geram milh√µes de linhas de log diariamente. Essas mensagens registram o comportamento interno do sistema: cada opera√ß√£o, cada erro, cada alerta.</p>
<p>O desafio √©: <strong>como detectar automaticamente que algo est√° errado, antes que o sistema falhe?</strong> A abordagem tradicional depende de regras manuais ("se aparecer a palavra ERROR, alerte"), mas isso √© fr√°gil e n√£o captura padr√µes sutis que antecedem falhas catastr√≥ficas.</p>

<h3>O que √© o LogGPT?</h3>
<p>O <strong>LogGPT</strong> √© um modelo de intelig√™ncia artificial baseado na arquitetura <strong>GPT-2</strong> (a mesma fam√≠lia do ChatGPT) adaptado especificamente para analisar logs de sistemas computacionais. Em vez de aprender a prever a pr√≥xima palavra em textos humanos, o LogGPT aprende a <strong>prever o pr√≥ximo evento de log</strong> em uma sequ√™ncia de opera√ß√µes do sistema.</p>

<div class="note">
üí° <strong>Analogia simples:</strong> Imagine um m√©dico que, ap√≥s anos observando batimentos card√≠acos normais, consegue identificar instantaneamente quando um ritmo est√° "fora do padr√£o". O LogGPT faz o mesmo com logs: ele aprende o padr√£o "saud√°vel" e identifica quando o sistema come√ßa a se comportar de forma an√¥mala.
</div>

<h3>Objetivo do Trabalho</h3>
<p>Este trabalho avalia a capacidade do LogGPT de:</p>
<ol>
<li><strong>Detectar anomalias</strong> ‚Äî identificar sess√µes que cont√™m falhas reais</li>
<li><strong>Antecipar falhas</strong> ‚Äî alertar <em>antes</em> do erro acontecer (lead time)</li>
<li><strong>Generalizar</strong> ‚Äî funcionar em diferentes dom√≠nios (cloud, storage, HPC)</li>
</ol>

<h3>Papers e Refer√™ncias</h3>
<table>
<tr><th>Refer√™ncia</th><th>Contribui√ß√£o</th></tr>
<tr><td>Guo et al. (2021) ‚Äî <em>"LogGPT: Log Anomaly Detection via GPT"</em></td><td>Proposta original do m√©todo, usando GPT-2 como modelo causal de linguagem para detec√ß√£o de anomalias em logs estruturados.</td></tr>
<tr><td>He et al. (2020) ‚Äî <em>"Loghub: A Large Collection of System Log Datasets"</em></td><td>Reposit√≥rio de datasets de logs usados como benchmark (OpenStack, HDFS, BGL).</td></tr>
<tr><td>Du et al. (2017) ‚Äî <em>"DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning"</em></td><td>Trabalho seminal que introduziu deep learning para detec√ß√£o de anomalias em logs, usando LSTM.</td></tr>
<tr><td>He et al. (2017) ‚Äî <em>"Drain: An Online Log Parsing Approach"</em></td><td>Algoritmo de parsing que converte logs brutos em templates estruturados (EventIds).</td></tr>
<tr><td>Radford et al. (2019) ‚Äî <em>"Language Models are Unsupervised Multitask Learners"</em> (GPT-2)</td><td>Arquitetura base do modelo, demonstrando que modelos causais de linguagem podem capturar padr√µes sequenciais complexos.</td></tr>
</table>
</section>

<!-- 2. METODOLOGIA -->
<section>
<h2>2. Metodologia ‚Äî Passo a Passo</h2>
<p>Abaixo explicamos cada etapa do processo, desde os logs brutos at√© a detec√ß√£o final, de forma que qualquer pessoa possa entender.</p>

<div class="img-c"><img src="data:image/png;base64,{c_pipe}" alt="Pipeline"></div>

<div class="step-box"><h4>Etapa 1 ‚Äî Coleta de Logs Brutos</h4>
<p>Os logs s√£o coletados diretamente dos servidores. Cada linha cont√©m um timestamp, n√≠vel de severidade (INFO, WARNING, ERROR), o componente que gerou o log e a mensagem em si.</p>
<p><code>2018-06-26 03:34:27 INFO nova.compute: Instance i-00000001 launched successfully</code></p>
</div>

<div class="step-box"><h4>Etapa 2 ‚Äî Log Parsing (Drain)</h4>
<p>O algoritmo <strong>Drain</strong> converte cada mensagem de log em um <strong>template</strong> (EventId), substituindo valores vari√°veis por wildcards <code>&lt;*&gt;</code>. Isso reduz milh√µes de mensagens √∫nicas para dezenas de templates reutiliz√°veis.</p>
<p>Exemplo: <code>"Instance i-00000001 launched"</code> ‚Üí Template: <code>"Instance &lt;*&gt; launched"</code> ‚Üí EventId: <code>e17b68d6</code></p>
</div>

<div class="step-box"><h4>Etapa 3 ‚Äî Agrupamento em Sess√µes</h4>
<p>Os logs s√£o agrupados por identificador de sess√£o:</p>
<ul>
<li><strong>OpenStack:</strong> por <code>test_id</code> (cada teste do Tempest gera uma sess√£o)</li>
<li><strong>HDFS:</strong> por <code>block_id</code> (cada bloco de dados tem sua sequ√™ncia)</li>
<li><strong>BGL:</strong> por <code>node_id</code> + janela temporal (sliding window de 20 eventos)</li>
</ul>
<p>Uma sess√£o vira uma sequ√™ncia de EventIds: <code>"e17b68d6 96691030 f7725eaf b8be6124"</code></p>
</div>

<div class="step-box"><h4>Etapa 4 ‚Äî Treinamento do Modelo (Causal LM)</h4>
<p>O modelo GPT-2 √© treinado <strong>apenas em sess√µes normais</strong> (sem falha). Ele aprende a prever "qual ser√° o pr√≥ximo evento?" dado o contexto anterior. Ap√≥s o treino, ele sabe qual √© o comportamento "normal" do sistema.</p>
<p><strong>Bibliotecas:</strong> PyTorch, HuggingFace Transformers, Polars (processamento de dados), Scikit-learn (m√©tricas).</p>
</div>

<div class="step-box"><h4>Etapa 5 ‚Äî Detec√ß√£o (Top-K)</h4>
<p>Na fase de detec√ß√£o, o modelo recebe cada sess√£o de teste e, para cada evento, verifica se o evento real est√° entre as <strong>Top-K predi√ß√µes mais prov√°veis</strong> (K=5). Se o evento real N√ÉO estiver no Top-5, o modelo marca aquele ponto como <strong>an√¥malo</strong>.</p>
<p>Se qualquer ponto da sess√£o for an√¥malo, toda a sess√£o √© classificada como an√¥mala.</p>
</div>

<div class="step-box"><h4>Etapa 6 ‚Äî C√°lculo do Lead Time</h4>
<p>O <strong>lead time</strong> mede quanto tempo <em>antes</em> do primeiro erro real o modelo detectou a anomalia. Usamos timestamps reais (resolu√ß√£o de microssegundos) para calcular essa diferen√ßa em minutos/horas.</p>
<p>Lead Time = Timestamp do 1¬∫ Erro Real ‚àí Timestamp da Detec√ß√£o pelo Modelo</p>
</div>

<h3>Tecnologias Utilizadas</h3>
<table>
<tr><th>Tecnologia</th><th>Vers√£o</th><th>Uso</th></tr>
<tr><td>Python</td><td>3.10+</td><td>Linguagem principal</td></tr>
<tr><td>PyTorch</td><td>2.x</td><td>Framework de deep learning</td></tr>
<tr><td>HuggingFace Transformers</td><td>4.x</td><td>Tokenizer e config do GPT-2</td></tr>
<tr><td>DistilGPT-2</td><td>‚Äî</td><td>Tokenizer base (vocabul√°rio de 50257 tokens)</td></tr>
<tr><td>Polars</td><td>0.20+</td><td>Processamento eficiente de DataFrames</td></tr>
<tr><td>Pandas</td><td>2.x</td><td>An√°lise de dados e gera√ß√£o de relat√≥rios</td></tr>
<tr><td>Scikit-learn</td><td>1.x</td><td>M√©tricas (precision, recall, F1, confusion matrix)</td></tr>
<tr><td>Matplotlib + Seaborn</td><td>3.x / 0.13</td><td>Visualiza√ß√µes e gr√°ficos</td></tr>
</table>

<h3>Implementa√ß√£o T√©cnica ‚Äî Par√¢metros e C√≥digo</h3>
<p>Abaixo detalhamos os hiperpar√¢metros escolhidos para cada dataset, com justificativa t√©cnica para cada decis√£o.</p>

<h4>Compara√ß√£o de Hiperpar√¢metros</h4>
<table>
<tr><th>Par√¢metro</th><th style="color:#27ae60">OpenStack</th><th style="color:#3498db">HDFS</th><th>Justificativa</th></tr>
<tr><td><strong>Tokenizer Base</strong></td><td><code>gpt2</code></td><td><code>distilgpt2</code></td><td>O OpenStack usa o tokenizer GPT-2 completo; HDFS usa DistilGPT-2 (mesma tokeniza√ß√£o, modelo menor) por quest√£o de performance no volume de dados (~575K sess√µes).</td></tr>
<tr><td><strong>BLOCK_SIZE</strong></td><td><code>1024</code></td><td><code>128</code></td><td>OpenStack tem sess√µes longas (m√©dia de 494 logs por teste) ‚Äî precisa de contexto grande. HDFS tem sess√µes curtas (2-20 eventos por bloco) ‚Äî 128 tokens √© mais que suficiente e otimiza mem√≥ria GPU.</td></tr>
<tr><td><strong>BATCH_SIZE</strong></td><td><code>8</code></td><td><code>64</code></td><td>OpenStack com BLOCK_SIZE=1024 consome ~12GB VRAM com batch de 8. HDFS com BLOCK_SIZE=128 permite batches 8x maiores, acelerando o treinamento na RTX 3080 Ti.</td></tr>
<tr><td><strong>EPOCHS</strong></td><td><code>10</code></td><td><code>30</code></td><td>OpenStack tem apenas 420 sess√µes ‚Äî 10 √©pocas s√£o suficientes para convergir sem overfitting. HDFS tem ~460K sess√µes de treino ‚Äî precisa de mais √©pocas para o modelo aprender padr√µes de blocos curtos.</td></tr>
<tr><td><strong>LEARNING_RATE</strong></td><td><code>1e-4</code></td><td><code>1e-4</code></td><td>Taxa de aprendizado conservadora. Valor padr√£o do paper original LogGPT que se mostrou est√°vel em ambos os datasets.</td></tr>
<tr><td><strong>N_LAYER / N_HEAD / N_EMBD</strong></td><td colspan="2"><code>4 / 4 / 256</code></td><td>Modelo "Small" com ~5M par√¢metros. 4 camadas e 4 cabe√ßas de aten√ß√£o capturam padr√µes sequenciais sem risco de overfitting em datasets menores.</td></tr>
<tr><td><strong>DROPOUT</strong></td><td colspan="2"><code>0.1</code></td><td>Regulariza√ß√£o leve (10% dos neur√¥nios desligados aleatoriamente) para evitar memoriza√ß√£o.</td></tr>
<tr><td><strong>K (Top-K)</strong></td><td colspan="2"><code>5</code></td><td>Se o pr√≥ximo evento real n√£o estiver entre as 5 predi√ß√µes mais prov√°veis do modelo, √© marcado como anomalia. K=5 equilibra sensibilidade (detectar anomalias sutis) vs especificidade (evitar falsos positivos). Valores menores (K=1) geram muitos falsos positivos; maiores (K=10) perdem anomalias sutis.</td></tr>
<tr><td><strong>SKIP_START_LOGS</strong></td><td><code>1</code></td><td><code>3</code></td><td>Ignora os N primeiros logs de cada sess√£o durante a detec√ß√£o ("cold start"). No OpenStack, anomalias podem aparecer logo no 2¬∫ evento (sess√µes de 7 logs); no HDFS, os primeiros 3 eventos s√£o sempre de aloca√ß√£o (previs√≠veis).</td></tr>
<tr><td><strong>LOG_COLUMN</strong></td><td><code>EventId</code></td><td><code>EventTemplate</code></td><td>OpenStack usa o hash curto do EventId (1-2 tokens); HDFS usa o template completo. A escolha impacta a tokeniza√ß√£o ‚Äî EventId produz sequ√™ncias mais compactas.</td></tr>
<tr><td><strong>SEED</strong></td><td colspan="2"><code>42</code></td><td>Semente fixa para reprodutibilidade total dos experimentos.</td></tr>
</table>

<h4>1. Processamento de Dados e Tokeniza√ß√£o (Polars e HuggingFace)</h4>
<p>O processamento de logs brutos em tensores para a GPU √© feito em duas etapas. Primeiro, usamos a biblioteca <strong>Polars</strong> (por sua velocidade e processamento multi-core em Rust) para agrupar milh√µes de linhas de log em "sess√µes". No OpenStack, agrupamos por <code>test_id</code> e concatenamos os <code>EventId</code> com espa√ßos:</p>
<pre style="background:rgba(0,0,0,.3);padding:20px;border-radius:8px;overflow-x:auto;font-size:12px;color:#e0e0e0;font-family:'Fira Code',monospace"><span style="color:#7f8c8d"># dataset.py ‚Äî Agrupamento de Sess√µes com Polars</span>
sessions = (
    df.sort(<span style="color:#27ae60">"timestamp"</span>)
    .group_by(<span style="color:#27ae60">"test_id"</span>)
    .agg([
        pl.col(LOG_COLUMN),
        pl.col(<span style="color:#27ae60">"anom_label"</span>).max().alias(<span style="color:#27ae60">"label"</span>) <span style="color:#7f8c8d"># Se 1 log for an√¥malo, a sess√£o inteira √©</span>
    ])
).with_columns(
    <span style="color:#7f8c8d"># Resultado: "E1 E2 E5 E1 E3..."</span>
    pl.col(LOG_COLUMN).list.join(<span style="color:#27ae60">" "</span>).alias(<span style="color:#27ae60">"EventTemplate"</span>)
)</pre>

<p>Em seguida, transformamos as strings em tensores PyTorch usando o Tokenizer do HuggingFace. Para otimizar mem√≥ria no treinamento de batches com tamanhos de sess√£o variados, usamos uma fun√ß√£o de cola√ß√£o (<code>collate_fn</code>) que faz <strong>Dynamic Padding</strong> na CPU antes de enviar para a GPU:</p>
<pre style="background:rgba(0,0,0,.3);padding:20px;border-radius:8px;overflow-x:auto;font-size:12px;color:#e0e0e0;font-family:'Fira Code',monospace"><span style="color:#7f8c8d"># dataset.py ‚Äî Dynamic Padding para batches de tamanho vari√°vel</span>
<span style="color:#e74c3c">def</span> <span style="color:#3498db">collate_fn</span>(batch):
    max_len = max(len(x) <span style="color:#e74c3c">for</span> x <span style="color:#e74c3c">in</span> batch)
    <span style="color:#7f8c8d"># Preenche com PAD_TOKEN (50256 no GPT2) at√© o maior log do batch atual</span>
    padded = torch.full((len(batch), max_len), <span style="color:#3498db">50256</span>, dtype=torch.long)
    <span style="color:#e74c3c">for</span> i, x <span style="color:#e74c3c">in</span> enumerate(batch):
        padded[i, :len(x)] = x
    <span style="color:#e74c3c">return</span> padded</pre>

<h4>2. Treinamento Causal LM (PyTorch)</h4>
<p>O treinamento do modelo n√£o usa labels bin√°rios (0 ou 1) de anomalia. O modelo √© treinado de forma auto-supervisionada (apenas em dados normais) usando <strong>Teacher Forcing</strong>: dado um contexto de N tokens, deve prever o token N+1. Isso √© feito atrav√©s do deslocamento de matrizes (<em>shift</em>):</p>
<pre style="background:rgba(0,0,0,.3);padding:20px;border-radius:8px;overflow-x:auto;font-size:12px;color:#e0e0e0;font-family:'Fira Code',monospace"><span style="color:#7f8c8d"># train_custom.py ‚Äî Loop de treinamento (Causal LM Shift)</span>
<span style="color:#e74c3c">def</span> <span style="color:#3498db">train_epoch</span>(model, loader, optimizer, device, epoch):
    model.train()
    <span style="color:#e74c3c">for</span> batch <span style="color:#e74c3c">in</span> loader:
        <span style="color:#7f8c8d"># Causal shift: alvo √© a entrada deslocada de 1 posi√ß√£o para a direita</span>
        inp = batch[:, :-<span style="color:#3498db">1</span>].to(device)  <span style="color:#7f8c8d"># Contexto (T_0 at√© T_N-1)</span>
        tgt = batch[:, <span style="color:#3498db">1</span>:].to(device)   <span style="color:#7f8c8d"># Alvo a prever (T_1 at√© T_N)</span>
        
        logits, loss = model(inp, targets=tgt)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step() <span style="color:#7f8c8d"># AdamW Optimizer com weight decay autom√°tico</span></pre>

<h4>3. Arquitetura do Modelo ‚Äî LogGPT Small</h4>
<p>O modelo usa uma arquitetura GPT-2 customizada ("LogGPT-Small") com as seguintes especifica√ß√µes:</p>
<pre style="background:rgba(0,0,0,.3);padding:20px;border-radius:8px;overflow-x:auto;font-size:12px;color:#e0e0e0;font-family:'Fira Code',monospace"><span style="color:#7f8c8d"># model.py ‚Äî Defini√ß√£o do modelo</span>
<span style="color:#e74c3c">class</span> <span style="color:#f39c12">LogGPT</span>(nn.Module):
    <span style="color:#7f8c8d">\"\"\"GPT-2 customizado para detec√ß√£o de anomalias em logs.\"\"\"</span>
    <span style="color:#e74c3c">def</span> __init__(self, config):
        self.transformer = GPT2Model(config)  <span style="color:#7f8c8d"># 4 layers, 4 heads, 256 embd</span>
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)
        
    <span style="color:#e74c3c">def</span> forward(self, input_ids):
        hidden = self.transformer(input_ids).last_hidden_state
        logits = self.lm_head(hidden)  <span style="color:#7f8c8d"># Shape: [batch, seq_len, vocab_size]</span>
        <span style="color:#e74c3c">return</span> logits</pre>

<h4>Detec√ß√£o Top-K ‚Äî C√≥digo Principal</h4>
<p>O trecho abaixo mostra a l√≥gica central de detec√ß√£o, id√™ntica para OpenStack e HDFS:</p>
<pre style="background:rgba(0,0,0,.3);padding:20px;border-radius:8px;overflow-x:auto;font-size:12px;color:#e0e0e0;font-family:'Fira Code',monospace"><span style="color:#7f8c8d"># detect_custom.py ‚Äî L√≥gica Top-K</span>
K = <span style="color:#3498db">5</span>  <span style="color:#7f8c8d"># Top-K parameter</span>

<span style="color:#7f8c8d"># 1. Forward pass pelo modelo</span>
logits, _ = model(input_ids)     <span style="color:#7f8c8d"># [batch, seq_len, vocab_size]</span>

<span style="color:#7f8c8d"># 2. Shift: comparar predi√ß√£o[i] com alvo[i+1]</span>
targets = input_ids[:, <span style="color:#3498db">1</span>:]       <span style="color:#7f8c8d"># O que realmente aconteceu</span>
preds   = logits[:, :-<span style="color:#3498db">1</span>, :]     <span style="color:#7f8c8d"># O que o modelo previu</span>

<span style="color:#7f8c8d"># 3. Calcular Top-K predi√ß√µes mais prov√°veis</span>
probs = torch.softmax(preds, dim=-<span style="color:#3498db">1</span>)
_, topk_inds = torch.topk(probs, K, dim=-<span style="color:#3498db">1</span>)

<span style="color:#7f8c8d"># 4. Verificar se o evento REAL est√° no Top-K</span>
matches = (topk_inds == targets.unsqueeze(-<span style="color:#3498db">1</span>)).any(dim=-<span style="color:#3498db">1</span>)

<span style="color:#7f8c8d"># 5. Anomalia = evento N√ÉO est√° no Top-K (e n√£o √© padding)</span>
valid_anomalies = (~matches) & target_mask

<span style="color:#7f8c8d"># 6. Sess√£o inteira √© an√¥mala se QUALQUER evento for</span>
is_anomalous = valid_anomalies.any(dim=<span style="color:#3498db">1</span>)</pre>

<h4>C√°lculo de Lead Time ‚Äî C√≥digo com Timestamps Reais</h4>
<p>O lead time √© calculado usando timestamps com resolu√ß√£o de microssegundos:</p>
<pre style="background:rgba(0,0,0,.3);padding:20px;border-radius:8px;overflow-x:auto;font-size:12px;color:#e0e0e0;font-family:'Fira Code',monospace"><span style="color:#7f8c8d"># Lead Time = Timestamp do 1¬∫ Erro Real ‚àí Timestamp da Detec√ß√£o</span>
<span style="color:#7f8c8d"># Positivo ‚Üí modelo ANTECIPOU a falha</span>
<span style="color:#7f8c8d"># Negativo ‚Üí modelo detectou DEPOIS (reativo)</span>

<span style="color:#e74c3c">if</span> pred_label == <span style="color:#3498db">1</span> <span style="color:#e74c3c">and</span> first_error_timestamp <span style="color:#e74c3c">is not None</span>:
    <span style="color:#7f8c8d"># Mapear o passo Top-K para timestamp real do evento</span>
    alert_ts = pd.to_datetime(session_timestamps[first_anomaly_step])
    error_ts = pd.to_datetime(first_error_timestamp)
    
    <span style="color:#7f8c8d"># Diferen√ßa em segundos (positivo = antecipa√ß√£o)</span>
    lead_time_seconds = (error_ts - alert_ts).total_seconds()
    lead_time_minutes = lead_time_seconds / <span style="color:#3498db">60.0</span></pre>

<div class="note">
üí° <strong>Por que timestamps reais?</strong> Inicialmente o lead time era medido em n√∫mero de eventos ("o modelo detectou 5 eventos antes do erro"). Por√©m, isso n√£o diz quanto TEMPO o operador teria para reagir. Com timestamps reais, sabemos que no OpenStack a antecipa√ß√£o m√©dia √© de <strong>3.8 minutos</strong> e no HDFS de at√© <strong>15 horas</strong>.
</div>

<h4>Justificativa das M√©tricas Escolhidas</h4>
<table>
<tr><th>M√©trica</th><th>Por que usamos</th><th>Limita√ß√£o</th></tr>
<tr><td><strong>F1-Score</strong></td><td>M√©trica principal. √â a m√©dia harm√¥nica de Precision e Recall ‚Äî penaliza modelos que sacrificam um pelo outro. Essencial quando os datasets s√£o desbalanceados (mais sess√µes normais que an√¥malas).</td><td>N√£o captura a distribui√ß√£o dos erros ‚Äî um F1 de 90% pode esconder que o modelo erra sempre no mesmo tipo de falha.</td></tr>
<tr><td><strong>Precision</strong></td><td>Crucial em produ√ß√£o: um sistema com baixa precision gera "fadiga de alertas" ‚Äî operadores ignoram alarmes quando muitos s√£o falsos.</td><td>Alta precision com baixo recall significa que falhas reais est√£o passando despercebidas.</td></tr>
<tr><td><strong>Recall</strong></td><td>Mede a capacidade do modelo de encontrar TODAS as falhas. Em sistemas cr√≠ticos (como um supercomputador), perder uma falha pode ser catastr√≥fico.</td><td>100% de recall √© f√°cil de atingir: basta alertar tudo (como o BGL fez com precision de 48.9%).</td></tr>
<tr><td><strong>Lead Time</strong></td><td>Diferencial do LogGPT: n√£o apenas DETECTA anomalias, mas ANTECIPA. Mede o tempo real entre a detec√ß√£o e o primeiro erro ‚Äî quanto maior, mais tempo para reagir.</td><td>Depende da resolu√ß√£o temporal dos timestamps. Datasets com timestamps imprecisos (ex: apenas data sem hora) impossibilitam c√°lculos granulares.</td></tr>
<tr><td><strong>Confusion Matrix</strong></td><td>Visualiza√ß√£o completa de TP/TN/FP/FN. Permite entender exatamente ONDE o modelo erra ‚Äî crucial para debugging e melhoria.</td><td>N√£o captura a severidade dos erros ‚Äî um FP em uma sess√£o de teste √© diferente de um FP em produ√ß√£o.</td></tr>
</table>
</section>

<!-- 3. DATASETS -->
<section>
<h2>3. Datasets ‚Äî Descri√ß√£o e Particularidades</h2>
<p>Utilizamos tr√™s datasets p√∫blicos do reposit√≥rio <a href="https://github.com/logpai/loghub" style="color:var(--blue)">Loghub</a>, cada um representando um cen√°rio distinto de infraestrutura computacional.</p>

<table>
<tr><th>Caracter√≠stica</th><th style="color:#27ae60">üü¢ OpenStack</th><th style="color:#3498db">üîµ HDFS</th><th style="color:#e74c3c">üî¥ BGL</th></tr>
<tr><td><strong>Dom√≠nio</strong></td><td>Cloud Computing (IaaS)</td><td>Armazenamento Distribu√≠do</td><td>Supercomputador HPC</td></tr>
<tr><td><strong>Fonte</strong></td><td>Loghub</td><td>Loghub</td><td>Loghub</td></tr>
<tr><td><strong>Per√≠odo</strong></td><td>Jun-Jul 2018</td><td>Nov 2008</td><td>Jun 2005 ‚Äî Jan 2006</td></tr>
<tr><td><strong>Total de Logs</strong></td><td>~424K linhas</td><td>~11M linhas</td><td>~4.7M linhas</td></tr>
<tr><td><strong>Sess√µes</strong></td><td>420 (test_ids)</td><td>~575K (block_ids)</td><td>~370K (sliding windows)</td></tr>
<tr><td><strong>Templates √önicos</strong></td><td>30</td><td>29</td><td><span class="badge br">242</span></td></tr>
<tr><td><strong>Agrupamento</strong></td><td>Por teste (test_id)</td><td>Por bloco HDFS (block_id)</td><td>Por n√≥ + janela temporal</td></tr>
<tr><td><strong>Tipos de Falha</strong></td><td>Erros de API, exce√ß√µes Python, timeouts</td><td>I/O exceptions, interrup√ß√µes</td><td>Hardware: mem√≥ria, cache, rede torus</td></tr>
<tr><td><strong>Resolu√ß√£o Temporal</strong></td><td>Microssegundos</td><td>Microssegundos</td><td>Segundos (Unix epoch)</td></tr>
<tr><td><strong>Modelo Usado</strong></td><td>Treinado localmente</td><td>Treinado localmente</td><td><span class="badge br">Modelo OpenStack (transfer√™ncia)</span></td></tr>
</table>

<h3>üü¢ OpenStack ‚Äî Cloud Computing</h3>
<p>O OpenStack √© uma plataforma open-source de cloud computing. O dataset cont√©m logs de testes automatizados (Tempest) que exercitam APIs de cria√ß√£o de inst√¢ncias, volumes, redes e imagens. As sess√µes s√£o relativamente curtas (dezenas a centenas de eventos) e os templates s√£o bem definidos. <strong>Ideal para o LogGPT</strong> pois os padr√µes sequenciais s√£o claros e consistentes.</p>

<h3>üîµ HDFS ‚Äî Hadoop Distributed File System</h3>
<p>O HDFS √© o sistema de arquivos distribu√≠do do Hadoop. Cada bloco de dados gera uma sequ√™ncia de log (aloca√ß√£o ‚Üí replica√ß√£o ‚Üí servir leituras). As falhas s√£o predominantemente de I/O (rede, disco). O dataset √© muito grande (~575K blocos), o que d√° ao modelo bastante dados para aprender. <strong>Desafio:</strong> muitas sess√µes muito curtas (2-5 eventos).</p>

<h3>üî¥ BGL ‚Äî Blue Gene/L Supercomputer</h3>
<p>O BGL √© um supercomputador IBM com 131.072 processadores. O dataset registra falhas de hardware: erros de mem√≥ria, cache, rede torus, kernel panics. √â fundamentalmente diferente dos outros dois datasets.</p>

<h4>‚ö†Ô∏è Diferen√ßa Estrutural Cr√≠tica: Como as sess√µes s√£o formadas</h4>
<p>A diferen√ßa mais importante entre os datasets est√° na <strong>forma como os logs s√£o agrupados em sess√µes</strong>:</p>
<table>
<tr><th>Dataset</th><th>Agrupamento</th><th>O que representa</th><th>Compat√≠vel com Causal LM?</th></tr>
<tr><td style="color:#27ae60"><strong>OpenStack</strong></td><td><code>test_id</code></td><td>Uma <strong>opera√ß√£o completa</strong> (teste Tempest) com in√≠cio, meio e fim definidos</td><td><span class="badge bg">‚úÖ Sim</span></td></tr>
<tr><td style="color:#3498db"><strong>HDFS</strong></td><td><code>block_id</code></td><td>O <strong>ciclo de vida</strong> de um bloco: aloca√ß√£o ‚Üí replica√ß√£o ‚Üí leitura</td><td><span class="badge bb">‚úÖ Sim</span></td></tr>
<tr><td style="color:#e74c3c"><strong>BGL</strong></td><td><code>node_id</code></td><td>Uma <strong>m√°quina f√≠sica</strong> ‚Äî acumula logs de meses de opera√ß√£o misturada</td><td><span class="badge br">‚ùå N√£o</span></td></tr>
</table>
<p>No OpenStack e HDFS, cada sess√£o √© um <strong>ciclo de vida completo de uma opera√ß√£o</strong> ‚Äî o modelo consegue aprender a sequ√™ncia "normal" (ex: criar VM ‚Üí configurar rede ‚Üí boot ‚Üí sucesso) e detectar desvios (ex: timeout no meio). No BGL, o <code>node_id</code> (ex: <code>R02-M1-N0-C:J12-U11</code>) √© apenas o endere√ßo de uma m√°quina f√≠sica que registra <strong>todos os tipos de eventos ao longo de meses</strong> sem nenhuma separa√ß√£o l√≥gica. N√£o h√° um "fluxo previs√≠vel" ‚Äî √© uma mistura ca√≥tica de eventos de hardware rotineiros e erros reais.</p>

<div class="warn">
‚ö†Ô∏è <strong>Conclus√£o da an√°lise estrutural:</strong> A abordagem Causal LM ("preveja o pr√≥ximo evento") s√≥ funciona quando os logs formam <strong>sequ√™ncias previs√≠veis com come√ßo, meio e fim</strong>. Datasets como OpenStack (<code>test_id</code>) e HDFS (<code>block_id</code>) naturalmente satisfazem essa condi√ß√£o. O BGL, por agrupar logs por m√°quina f√≠sica (<code>node_id</code>), <strong>n√£o possui essa propriedade</strong>, tornando a abordagem LogGPT fundamentalmente inadequada para este tipo de dado ‚Äî independentemente de re-treinamento.
</div>
</section>

<!-- 4. RESULTADOS COMPARATIVOS -->
<section>
<h2>4. Resultados Comparativos</h2>

<div class="kpi-row">
<div class="kpi green"><div class="l">OpenStack F1</div><div class="v">{OS['f1']*100:.1f}%</div><div class="l">Precision {OS['precision']*100:.1f}% ¬∑ Recall {OS['recall']*100:.1f}%</div></div>
<div class="kpi blue"><div class="l">HDFS F1</div><div class="v">{HD['f1']*100:.1f}%</div><div class="l">Precision {HD['precision']*100:.1f}% ¬∑ Recall {HD['recall']*100:.1f}%</div></div>
<div class="kpi red"><div class="l">BGL F1</div><div class="v">{BG['f1']*100:.1f}%</div><div class="l">Precision {BG['precision']*100:.1f}% ¬∑ Recall {BG['recall']*100:.1f}%</div></div>
</div>

<h3>4.1 M√©tricas de Classifica√ß√£o</h3>
<div class="img-c"><img src="data:image/png;base64,{c_metrics}" alt="M√©tricas"></div>

<h3>4.2 Radar Comparativo</h3>
<div class="img-c"><img src="data:image/png;base64,{c_radar}" alt="Radar"></div>

<h3>4.3 Matrizes de Confus√£o</h3>
<div class="img-c"><img src="data:image/png;base64,{c_cm}" alt="Confusion Matrix"></div>

<h3>Interpreta√ß√£o das M√©tricas</h3>
<table>
<tr><th>M√©trica</th><th>O que significa (linguagem simples)</th></tr>
<tr><td><strong>Precision</strong></td><td>"Dos alarmes que o modelo disparou, quantos eram falhas reais?" ‚Äî Alta precision = poucos alarmes falsos.</td></tr>
<tr><td><strong>Recall</strong></td><td>"Das falhas reais que existiam, quantas o modelo encontrou?" ‚Äî Alto recall = poucas falhas escaparam.</td></tr>
<tr><td><strong>F1-Score</strong></td><td>M√©dia harm√¥nica de Precision e Recall. √â a m√©trica principal ‚Äî quanto maior, melhor o equil√≠brio.</td></tr>
<tr><td><strong>Accuracy</strong></td><td>"No geral, quantas classifica√ß√µes estavam corretas?" ‚Äî Pode ser enganosa se os dados forem desbalanceados.</td></tr>
</table>
</section>

<!-- 5. OPENSTACK DETALHADO -->
<section>
<h2>5. OpenStack ‚Äî An√°lise Detalhada</h2>
<div class="kpi-row">
<div class="kpi green"><div class="l">True Positives</div><div class="v">{OS['tp']}</div></div>
<div class="kpi blue"><div class="l">True Negatives</div><div class="v">{OS['tn']}</div></div>
<div class="kpi gold"><div class="l">False Positives</div><div class="v">{OS['fp']}</div></div>
<div class="kpi red"><div class="l">False Negatives</div><div class="v">{OS['fn']}</div></div>
</div>
<p>O OpenStack obteve os <strong>melhores resultados</strong> entre os tr√™s datasets. Com apenas <strong>5 falsos positivos</strong> e <strong>7 falsos negativos</strong> em 420 sess√µes, o modelo demonstra uma capacidade excepcional de distinguir opera√ß√µes normais de falhas reais.</p>
<p>Os tipos de falha detectados incluem: erros de API REST (HTTP 500), exce√ß√µes Python (<code>TypeError</code>, <code>KeyError</code>), timeouts de opera√ß√µes e falhas de cria√ß√£o/destrui√ß√£o de recursos cloud.</p>

<h3>Lead Time ‚Äî Antecipa√ß√£o</h3>
<div class="kpi-row">
<div class="kpi green"><div class="l">Antecipa√ß√£o M√©dia</div><div class="v">{fmt(OS['lt_mean_min'])}</div></div>
<div class="kpi blue"><div class="l">Mediana</div><div class="v">{fmt(OS['lt_median_min'])}</div></div>
<div class="kpi gold"><div class="l">M√°xima</div><div class="v">{fmt(OS['lt_max_min'])}</div></div>
<div class="kpi green"><div class="l">% Antecipadas</div><div class="v">{OS['lt_pct_ant']:.0f}%</div></div>
</div>
<p>Em <strong>{OS['lt_pct_ant']:.0f}%</strong> das sess√µes detectadas, o modelo alertou <em>antes</em> do primeiro erro real ‚Äî com uma m√©dia de <strong>{fmt(OS['lt_mean_min'])}</strong> de anteced√™ncia. Isso demonstra que o LogGPT n√£o apenas detecta falhas, mas as <strong>antecipa</strong>, dando tempo para a√ß√µes corretivas antes que o impacto se materialize.</p>
</section>

<!-- 6. HDFS DETALHADO -->
<section>
<h2>6. HDFS ‚Äî An√°lise Detalhada</h2>
<div class="kpi-row">
<div class="kpi green"><div class="l">True Positives</div><div class="v">{HD['tp']:,}</div></div>
<div class="kpi blue"><div class="l">True Negatives</div><div class="v">{HD['tn']:,}</div></div>
<div class="kpi gold"><div class="l">False Positives</div><div class="v">{HD['fp']:,}</div></div>
<div class="kpi red"><div class="l">False Negatives</div><div class="v">{HD['fn']:,}</div></div>
</div>
<p>O HDFS processou <strong>{HD['total_sessions']:,} sess√µes de teste</strong> ‚Äî uma escala 170x maior que o OpenStack. Mesmo assim, manteve <strong>Precision de 95%</strong> com <strong>Recall de 82.3%</strong>. Os 2.983 falsos negativos representam blocos onde a anomalia era sutil demais para o modelo Top-K capturar (sess√µes muito curtas com poucos eventos).</p>

<h3>Lead Time ‚Äî Antecipa√ß√£o</h3>
<div class="kpi-row">
<div class="kpi green"><div class="l">Antecipa√ß√£o M√©dia</div><div class="v">{fmt(HD['lt_mean_min'])}</div></div>
<div class="kpi blue"><div class="l">Mediana</div><div class="v">{fmt(HD['lt_median_min'])}</div></div>
<div class="kpi gold"><div class="l">M√°xima</div><div class="v">{fmt(HD['lt_max_min'])}</div></div>
<div class="kpi green"><div class="l">% Antecipadas</div><div class="v">{HD['lt_pct_ant']:.0f}%</div></div>
</div>
<p>No HDFS, o modelo conseguiu antecipar falhas com at√© <strong>{fmt(HD['lt_max_min'])}</strong> de anteced√™ncia. A m√©dia de <strong>{fmt(HD['lt_mean_min'])}</strong> indica que, em muitos casos, o modelo detecta padr√µes pr√©-falha horas antes da cascata de I/O errors se materializar.</p>
</section>

<!-- 7. BGL ‚Äî POR QUE FALHOU -->
<section>
<h2>7. BGL ‚Äî An√°lise e Motivos do Insucesso</h2>
<div class="kpi-row">
<div class="kpi red"><div class="l">Precision</div><div class="v">{BG['precision']*100:.1f}%</div><div class="l">Quase metade s√£o alarmes falsos</div></div>
<div class="kpi gold"><div class="l">Recall</div><div class="v">{BG['recall']*100:.0f}%</div><div class="l">Encontrou tudo (pois alertou tudo)</div></div>
<div class="kpi red"><div class="l">F1-Score</div><div class="v">{BG['f1']*100:.1f}%</div><div class="l">Muito abaixo do aceit√°vel</div></div>
</div>

<h3>O que aconteceu?</h3>
<p>O BGL obteve <strong>100% de recall</strong> mas apenas <strong>48.9% de precision</strong>. Isso significa que o modelo <strong>classificou praticamente TODAS as sess√µes como an√¥malas</strong>, acertando as que realmente eram an√¥malas mas tamb√©m gerando uma quantidade massiva de falsos positivos. Existem duas causas ra√≠z combinadas:</p>

<h3>Causa 1 ‚Äî Modelo treinado no dom√≠nio errado (Transfer Learning)</h3>
<p>O modelo foi treinado exclusivamente com logs de <strong>OpenStack</strong> (software de cloud) e testado em logs de <strong>BGL</strong> (hardware de supercomputador). S√£o vocabul√°rios completamente diferentes:</p>
<ul>
<li><strong>OpenStack:</strong> HTTP requests, API calls, inst√¢ncias de VMs, opera√ß√µes CRUD</li>
<li><strong>BGL:</strong> Erros de mem√≥ria DDR, parity errors, cache ECC, rede torus, kernel panics</li>
</ul>
<p>O modelo nunca viu esses tipos de eventos durante o treinamento, ent√£o <strong>qualquer sequ√™ncia do BGL parece "an√¥mala"</strong> para ele.</p>

<h4>Diversidade de Templates</h4>
<div class="img-c"><img src="data:image/png;base64,{c_tmpl}" alt="Templates"></div>
<p>O BGL possui <strong>242 templates √∫nicos</strong> ‚Äî 8 vezes mais que o OpenStack (30) ou HDFS (29). Nenhum deles foi visto pelo modelo durante o treinamento.</p>

<h3>Causa 2 ‚Äî Estrutura de sess√£o incompat√≠vel (Problema Fundamental)</h3>
<p>Mesmo que re-trein√°ssemos o modelo com dados nativos do BGL, <strong>a abordagem LogGPT ainda n√£o funcionaria</strong>, porque a estrutura dos logs do BGL √© fundamentalmente incompat√≠vel com o m√©todo Causal LM.</p>

<h4>üîë A diferen√ßa crucial: o que √© uma "sess√£o"</h4>
<table>
<tr><th>Dataset</th><th>ID da Sess√£o</th><th>O que representa</th><th>Padr√£o sequencial?</th></tr>
<tr><td style="color:#27ae60"><strong>OpenStack</strong></td><td><code>test_id</code></td><td>Uma opera√ß√£o completa com in√≠cio ‚Üí meio ‚Üí fim</td><td><span class="badge bg">‚úÖ Previs√≠vel</span></td></tr>
<tr><td style="color:#3498db"><strong>HDFS</strong></td><td><code>block_id</code></td><td>Ciclo de vida do bloco: aloca√ß√£o ‚Üí replica√ß√£o ‚Üí leitura</td><td><span class="badge bb">‚úÖ Previs√≠vel</span></td></tr>
<tr><td style="color:#e74c3c"><strong>BGL</strong></td><td><code>node_id</code></td><td>M√°quina f√≠sica ‚Äî meses de logs misturados sem separa√ß√£o</td><td><span class="badge br">‚ùå Ca√≥tico</span></td></tr>
</table>

<p>No OpenStack, um <code>test_id</code> como <code>"nova.compute.test_create_instance"</code> representa um <strong>teste completo</strong>: criar VM ‚Üí configurar rede ‚Üí fazer boot ‚Üí verificar status ‚Üí limpar. O modelo aprende essa sequ√™ncia "healthy" e detecta quando algo desvia (ex: timeout no meio).</p>

<p>No HDFS, um <code>block_id</code> como <code>"blk_-1608999687919862906"</code> tem um <strong>ciclo de vida natural</strong>: o bloco √© alocado, replicado em 3 n√≥s, e depois servido para leituras. O modelo aprende essa cadeia e detecta quando um bloco falha no meio.</p>

<p>No BGL, um <code>node_id</code> como <code>"R02-M1-N0-C:J12-U11"</code> √© simplesmente <strong>o endere√ßo de um n√≥ f√≠sico do supercomputador</strong>. Ele acumula TODOS os logs daquela m√°quina ao longo de <strong>7 meses de opera√ß√£o</strong> (jun/2005 a jan/2006). N√£o existe um "fluxo" ‚Äî √© uma mistura ca√≥tica de:</p>
<ul>
<li>Eventos de hardware corriqueiros (corre√ß√µes de ECC, bit steering)</li>
<li>Erros reais (kernel panics, falhas de mem√≥ria)</li>
<li>Mensagens de manuten√ß√£o (reinicializa√ß√µes, atualiza√ß√µes)</li>
<li>Processos de diferentes aplica√ß√µes rodando simultaneamente</li>
</ul>

<div class="note">
üí° <strong>Analogia:</strong> Imagine que voc√™ quer que um m√©dico identifique batimentos card√≠acos irregulares. No OpenStack e HDFS, ele recebe um exame de ECG completo (come√ßo, meio, fim ‚Äî uma sequ√™ncia clara). No BGL, ele recebe <strong>7 meses de registros misturados</strong> de press√£o, temperatura, batimento, sono, exerc√≠cio, tudo junto e fora de ordem. N√£o h√° como aprender um "padr√£o normal" nesse caos.
</div>

<p>Para tentar contornar isso, dividimos os logs do BGL em <strong>janelas deslizantes de 20 eventos</strong> (sliding window). Mas isso √© artificial e:</p>
<ul>
<li>Quebra o contexto temporal (uma janela pode conter metade de um incidente)</li>
<li>Mistura eventos de diferentes origens na mesma janela</li>
<li>N√£o captura rela√ß√µes de longo prazo entre eventos do mesmo n√≥</li>
</ul>

<div class="warn">
‚ö†Ô∏è <strong>Conclus√£o BGL:</strong> O insucesso no BGL n√£o √© apenas uma quest√£o de re-treinamento. A abordagem Causal LM ("preveja o pr√≥ximo evento na sequ√™ncia") <strong>depende fundamentalmente de sess√µes com padr√µes sequenciais previs√≠veis</strong>. O BGL n√£o possui essa propriedade ‚Äî seus logs s√£o agrupados por m√°quina f√≠sica, n√£o por opera√ß√£o l√≥gica. Para datasets com essa estrutura, abordagens alternativas como <strong>frequ√™ncia de templates por janela temporal</strong>, <strong>grafos de depend√™ncia de hardware</strong>, ou <strong>modelos de s√©ries temporais</strong> seriam mais adequadas.
</div>
</section>

<!-- 8. LEAD TIME COMPARATIVO -->
<section>
<h2>8. Lead Time ‚Äî An√°lise Comparativa de Antecipa√ß√£o</h2>
<p>O <strong>lead time</strong> √© a capacidade preditiva mais valiosa do LogGPT: quanto tempo antes do primeiro erro real o modelo consegue alertar sobre a anomalia.</p>

<div class="img-c"><img src="data:image/png;base64,{c_lt}" alt="Lead Time"></div>

<table>
<tr><th>M√©trica de Lead Time</th><th style="color:#27ae60">OpenStack</th><th style="color:#3498db">HDFS</th><th style="color:#e74c3c">BGL</th></tr>
<tr><td>M√©dia (sess√µes antecipadas)</td><td><strong>{fmt(OS['lt_mean_min'])}</strong></td><td><strong>{fmt(HD['lt_mean_min'])}</strong></td><td>‚Äî</td></tr>
<tr><td>Mediana</td><td>{fmt(OS['lt_median_min'])}</td><td>{fmt(HD['lt_median_min'])}</td><td>‚Äî</td></tr>
<tr><td>M√°ximo</td><td>{fmt(OS['lt_max_min'])}</td><td>{fmt(HD['lt_max_min'])}</td><td>‚Äî</td></tr>
<tr><td>% Sess√µes Antecipadas</td><td>{OS['lt_pct_ant']:.0f}%</td><td>{HD['lt_pct_ant']:.0f}%</td><td>N/A (modelo inv√°lido)</td></tr>
</table>

<div class="note">
üìä <strong>Interpreta√ß√£o:</strong> No OpenStack, o modelo tipicamente antecipa falhas em ~3.5 minutos ‚Äî tempo suficiente para um sistema de automa√ß√£o acionar re-tentativas ou failover. No HDFS, a antecipa√ß√£o pode chegar a horas, permitindo realoca√ß√£o proativa de blocos de dados antes que falhas de disco se consumem.
</div>
</section>

<!-- 9. CONCLUS√ïES -->
<section>
<h2>9. Conclus√µes e Contribui√ß√µes</h2>

<h3>‚úÖ Contribui√ß√µes Positivas</h3>
<ol>
<li><strong>Valida√ß√£o do LogGPT como abordagem vi√°vel</strong> para detec√ß√£o proativa de anomalias em dois dom√≠nios (cloud computing e storage distribu√≠do).</li>
<li><strong>Capacidade de antecipa√ß√£o comprovada</strong>: o modelo n√£o apenas detecta, mas prev√™ falhas minutos ou horas antes de sua materializa√ß√£o.</li>
<li><strong>Pipeline reprodut√≠vel</strong>: todo o c√≥digo est√° documentado e dispon√≠vel para replica√ß√£o.</li>
<li><strong>An√°lise comparativa robusta</strong>: tr√™s datasets p√∫blicos testados, com m√©tricas completas e transparentes.</li>
</ol>

<h3>‚ö†Ô∏è Limita√ß√µes Identificadas</h3>
<ol>
<li><strong>Transfer√™ncia cross-domain limitada:</strong> O modelo treinado em um dom√≠nio (OpenStack) n√£o generaliza para hardware (BGL). √â necess√°rio re-treinar para novos dom√≠nios.</li>
<li><strong>Sensibilidade ao vocabul√°rio:</strong> A diversidade de templates impacta diretamente o desempenho. Datasets com muitos templates √∫nicos (&gt;100) s√£o desafiadores.</li>
<li><strong>Granularidade de sess√£o:</strong> A forma como os logs s√£o agrupados (sess√µes naturais vs. janelas deslizantes) afeta significativamente a qualidade da detec√ß√£o.</li>
</ol>

<h3>üîÆ Trabalhos Futuros</h3>
<ul>
<li>Retreinar o LogGPT diretamente em logs de BGL para avaliar se funciona com dados nativos do dom√≠nio.</li>
<li>Explorar embeddings de templates (Word2Vec, BERT) para melhorar a captura sem√¢ntica.</li>
<li>Implementar um sistema de re-treinamento cont√≠nuo (online learning) para adaptar o modelo a drift de vocabul√°rio.</li>
<li>Integrar com sistemas de orquestra√ß√£o (Kubernetes, Prometheus) para resposta autom√°tica.</li>
</ul>
</section>

<!-- 10. RESUMO FINAL -->
<section>
<h2>10. Tabela Resumo Final</h2>
<table>
<tr><th>Aspecto</th><th style="color:#27ae60">üü¢ OpenStack</th><th style="color:#3498db">üîµ HDFS</th><th style="color:#e74c3c">üî¥ BGL</th></tr>
<tr><td><strong>F1-Score</strong></td><td style="color:#27ae60;font-weight:bold">{OS['f1']*100:.1f}%</td><td style="color:#3498db;font-weight:bold">{HD['f1']*100:.1f}%</td><td style="color:#e74c3c;font-weight:bold">{BG['f1']*100:.1f}%</td></tr>
<tr><td><strong>Precision</strong></td><td>{OS['precision']*100:.1f}%</td><td>{HD['precision']*100:.1f}%</td><td>{BG['precision']*100:.1f}%</td></tr>
<tr><td><strong>Recall</strong></td><td>{OS['recall']*100:.1f}%</td><td>{HD['recall']*100:.1f}%</td><td>{BG['recall']*100:.0f}%</td></tr>
<tr><td><strong>Lead Time M√©dio</strong></td><td>{fmt(OS['lt_mean_min'])}</td><td>{fmt(HD['lt_mean_min'])}</td><td>N/A</td></tr>
<tr><td><strong>Templates</strong></td><td>{OS['n_templates']}</td><td>{HD['n_templates']}</td><td>{BG['n_templates']}</td></tr>
<tr><td><strong>Modelo</strong></td><td>Treinado local</td><td>Treinado local</td><td>Transfer (OpenStack)</td></tr>
<tr><td><strong>Veredicto</strong></td><td><span class="badge bg">‚úÖ Excelente</span></td><td><span class="badge bb">‚úÖ Bom</span></td><td><span class="badge br">‚ùå Insuficiente</span></td></tr>
</table>
</section>

</div>
</body></html>"""
