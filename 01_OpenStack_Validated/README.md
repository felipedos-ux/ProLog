# LogGPT-Small: Predi√ß√£o de Falhas com LLM Customizado

Modelo de linguagem customizado (30M par√¢metros) para detec√ß√£o e antecipa√ß√£o de falhas em logs.

## ü§ñ Modelo LLM

**Arquitetura**: GPT-2 Customizado (from scratch)
- **Par√¢metros**: ~30M
- **Layers**: 4 transformer blocks
- **Attention Heads**: 4
- **Embedding Dimension**: 256
- **Context Window**: 128 tokens
- **Tokenizer**: [DistilGPT2](https://huggingface.co/distilgpt2) (vocab_size: 50,257)

**Treinamento**:
- Dataset: Logs normais do OpenStack (80% dos dados)
- Objetivo: Causal Language Modeling (predi√ß√£o do pr√≥ximo token)
- Framework: PyTorch + Transformers

## üìä Resultados

- **Taxa de Antecipa√ß√£o**: 88.2% (149/169 falhas)
- **Lead Time M√©dio**: 17.70 minutos
- **Lead Time M√°ximo**: 27.88 minutos
- **Recall**: 100%
- **F1-Score**: 0.8848

## üìÅ Arquivos

- `model.py`: Arquitetura GPT customizada (4 layers, 4 heads, 256 dim)
- `dataset.py`: Prepara√ß√£o de dados (tokeniza√ß√£o, chunking)
- `train_custom.py`: Script de treinamento
- `detect_custom.py`: Script de detec√ß√£o e c√°lculo de lead time
- `model_weights/`: Modelo treinado (checkpoint)

## üöÄ Como Usar

### Treinar Modelo
```bash
python train_custom.py
```

**Configura√ß√£o**:
- Batch Size: 8
- √âpocas: 10
- Learning Rate: 3e-4
- Tempo: ~10 minutos (GPU)

### Executar Detec√ß√£o
```bash
python detect_custom.py
```

**Sa√≠da**:
- M√©tricas de classifica√ß√£o (F1, Precision, Recall)
- Lead times por sess√£o
- Top 10 melhores/piores antecipa√ß√µes
- An√°lise de diversidade de falhas

## üîß Requisitos

**Treinamento**:
- GPU: NVIDIA RTX 3080 Ti (12GB) ou superior
- RAM: 16GB

**Produ√ß√£o**:
- CPU: 4 cores @ 2.5GHz (GPU opcional)
- RAM: 4GB
- Lat√™ncia: < 1s por sess√£o

## üìñ Documenta√ß√£o Completa

Ver `../reports/loggpt_relatorio_detalhado.md` para:
- Algoritmo de detec√ß√£o passo-a-passo
- Explica√ß√£o do c√°lculo de lead time
- Exemplos pr√°ticos com c√≥digo
