# Plano de Melhoria: LogGPT-Small

**Data**: 06/02/2026  
**Baseado em**: `VALIDACAO_PROLOG_COMPLETA.md`

---

## üìã Resumo dos Problemas

| # | Problema | Prioridade | Status |
|---|----------|------------|--------|
| 1 | Threshold hardcoded (5.0) | üî¥ CR√çTICO | ‚è≥ Pendente |
| 2 | Validation set n√£o utilizado | üî¥ CR√çTICO | ‚è≥ Pendente |
| 3 | Loss values id√™nticos (17.43) | üî¥ CR√çTICO | ‚è≥ Pendente |
| 4 | Sem valida√ß√£o durante treino | üü° ALTO | ‚è≥ Pendente |
| 5 | Sem early stopping | üü° ALTO | ‚è≥ Pendente |
| 6 | Separa√ß√£o de logs fraca | üü¢ M√âDIO | ‚è≥ Pendente |
| 7 | Hiperpar√¢metros n√£o documentados | üü¢ M√âDIO | ‚è≥ Pendente |
| 8 | M√©tricas incompletas (sem AUC) | üü¢ M√âDIO | ‚è≥ Pendente |

---

## üî¥ Corre√ß√µes CR√çTICAS

### 1. Calibrar Threshold no Validation Set

**Problema**: Threshold 5.0 hardcoded sem calibra√ß√£o formal.

**Solu√ß√£o**: Criar `calibrate_threshold.py`

```python
# Pseudoc√≥digo
for threshold in np.arange(1.0, 20.0, 0.5):
    metrics = evaluate_on_val_set(model, val_ids, threshold)
    if metrics['f1'] > best_f1:
        best_threshold = threshold

# Salvar threshold √≥timo
save_threshold(best_threshold)  # ‚Üí optimal_threshold.txt
```

**Entreg√°veis**:
- [ ] `calibrate_threshold.py`
- [ ] `optimal_threshold.txt`
- [ ] Curva ROC (imagem)
- [ ] Curva Precision-Recall (imagem)

---

### 2. Usar Validation Set

**Problema**: `val_ids` calculado mas nunca usado.

**Solu√ß√£o**: Modificar `detect_custom.py`

```python
# ANTES
val_ids, test_norm_ids = train_test_split(...)  # val_ids ignorado

# DEPOIS
# 1. Calibrar no val_ids
threshold = calibrate_threshold(model, val_ids)

# 2. Avaliar no test_norm_ids + anom_ids
results = evaluate(model, test_ids, threshold)
```

---

### 3. Investigar Loss Id√™ntico

**Problema**: Todos 169 casos t√™m loss = 17.43 (estatisticamente imposs√≠vel).

**Diagn√≥stico Necess√°rio**:

```python
# Adicionar logging detalhado
for i, log in enumerate(session_logs):
    loss = calculate_loss(log)
    print(f"Log {i}: loss={loss:.6f}, tokens={len(new_ids)}")
    
    all_losses.append(loss)

# Plotar distribui√ß√£o
plt.hist(all_losses, bins=50)
plt.savefig("loss_distribution.png")
```

**Hip√≥teses**:
1. Bug no c√°lculo de `logit_indices`
2. Bug no `target_start_idx`
3. Modelo degenerado (sempre prediz mesmo token)

---

## üü° Corre√ß√µes ALTAS

### 4. Valida√ß√£o Durante Treino

**Problema**: Sem val_loss, imposs√≠vel detectar overfitting.

**Solu√ß√£o**: Modificar `train_custom.py`

```python
for epoch in range(EPOCHS):
    # Treino
    train_loss = train_epoch(model, train_loader)
    
    # Valida√ß√£o (NOVO)
    val_loss = evaluate_epoch(model, val_loader)
    
    print(f"Epoch {epoch}: Train={train_loss:.4f}, Val={val_loss:.4f}")
    
    # Salvar curva
    train_losses.append(train_loss)
    val_losses.append(val_loss)

# Plotar learning curve
plot_learning_curve(train_losses, val_losses)
```

---

### 5. Early Stopping

**Problema**: 10 epochs fixo sem justificativa.

**Solu√ß√£o**:

```python
patience = 3
best_val_loss = float('inf')
counter = 0

for epoch in range(MAX_EPOCHS):
    val_loss = evaluate(model, val_loader)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), "best_model.pt")
        counter = 0
    else:
        counter += 1
        if counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
```

---

## üü¢ Corre√ß√µes M√âDIAS

### 6. Melhorar Separa√ß√£o de Logs

**Atual**: `" \n ".join(logs)`

**Proposta**: Usar token especial `<|LOG|>`

```python
# dataset.py
LOG_SEPARATOR = " <|LOG|> "
session_text = LOG_SEPARATOR.join(logs)
```

---

### 7. Documentar Hiperpar√¢metros

Adicionar ao README:

| Par√¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| block_size | 128 | M√©dia de tokens por sess√£o |
| epochs | 10 | Converg√™ncia observada |
| batch_size | 32 | Limite GPU |
| learning_rate | 5e-4 | Padr√£o AdamW |

---

### 8. Adicionar M√©tricas

```python
from sklearn.metrics import roc_auc_score, average_precision_score

# Coletar scores cont√≠nuos
y_true = [...]  # 0/1 labels
y_scores = [...]  # max_loss por sess√£o

# Calcular
auc_roc = roc_auc_score(y_true, y_scores)
auc_pr = average_precision_score(y_true, y_scores)
```

---

## üìÖ Cronograma

| Semana | Tarefas | Esfor√ßo |
|--------|---------|---------|
| **1** | Problemas 1-3 (Cr√≠ticos) | 8h |
| **2** | Problemas 4-5 (Altos) + Retreino | 6h |
| **3** | Problemas 6-8 (M√©dios) | 4h |
| **4** | Documenta√ß√£o + Revis√£o Final | 4h |

**Total**: ~22 horas

---

## ‚úÖ Checklist de Valida√ß√£o

### Antes de Publica√ß√£o

- [ ] Threshold calibrado no validation set
- [ ] Bug de loss investigado e corrigido
- [ ] Valida√ß√£o durante treino implementada
- [ ] Early stopping implementado
- [ ] Curva de aprendizado plotada
- [ ] Curva ROC inclu√≠da
- [ ] Precision-Recall curve inclu√≠da
- [ ] README atualizado com hiperpar√¢metros
- [ ] Resultados revalidados no test set

---

## üìö Refer√™ncias

1. Fawcett (2006) - "An introduction to ROC analysis"
2. Prechelt (1998) - "Early Stopping - But When?"
3. Kaufman et al. (2012) - "Leakage in Data Mining"
